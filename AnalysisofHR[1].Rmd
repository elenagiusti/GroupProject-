---
title: "Analysis of Hr indicators as predictor for employees leaving"
author: "Giusti Elena,Sarah Wong,Zohaib Gulzar, Tatsuya Nagata"
date: ""
output: html_document
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
getwd()
source("CopyOfAnalyticsLibraries/library.R")
source("CopyOfAnalyticsLibraries/heatmapOutput.R")
library(corrplot)
library(randomForest)
library(readr)
library(psych)
library(corrplot)
library(dplyr)
library(tidyr)
library(tabplot)
library(ggplot2)
library(gridExtra)
library(knitr)
library(vcd)
library(caret)
library(randomForest)
library(partykit)
library(dplyr)
library(ggvis)
library(DT)
```



#Human Resources Analytics
##How do we retain our best and most experienced employees?

<br>Our dataset includes 14.999 observations, each line represent one single employee

<br>Fields in the dataset include the following 10 variables for each line:

<br/>-Employee satisfaction level
<br/>-Last evaluation: 
<br/>-Number of projects
<br/>-Average monthly hours
<br/>-Time spent at the company
<br/>-Whether they have had a work accident
<br/>-Whether they have had a promotion in the last 5 years
<br/>-Department
<br/>-Salary
<br/>-Whether the employee has left

###The objectives of the project are the following

<br/>1) Assess what are the relationship between the 10 variables and what are the significant variables to describe the dataset
<br/>2) Undestand who are the employees that have left
<br/>3) Focus the analysis on the most valuable employees who have left
<br/>4) Devolop a predictive model to assess the likelihood of an employee leaving 

###The project is divided in the following steps
<br>Step 1) Data quality check
<br>Step 2) Basic Data Visualisation
<br>Step 3) Principal Component Analysis
<br>Step 4) Futher comparative analysis on employees that left
<br>Step 5) Prediction Model
<br> Step 7) Conclusion

##Step 1: Data quality check:as a first step we will perform basic statistical analysis and understand the type of factors.

```{r, echo=FALSE,eval=FALSE}
knitr::opts_chunk$set(echo = TRUE)
myData = read.table("DATA/HR_comma_sep.csv", header = T,sep=',')
head(myData)
View(myData, header=T)
colnames(myData)
print(myData[1:10])

```
Looking at data we can make the following assumptions related the nature of the 10 variables:

<br/>satisfaction_level: A numeric indicator filled out by the employee ranging from 0 to 1
<br/>last_evaluation: A numeric indicator filled in by the employeeâ€™s manager ranging from 0 to 1
<br/>number_project: An integer that indicates the number of projects the employee has worked on
<br/>average_monthly_hours: The number of hours employees work in the month
<br/>time_spend_company: An integer value indicated the years of service
<br/>Work_accident: A dummy variable assessing whether(1) or not (0) they had an accident
<br/>left: A dummy variable, leave (1), not leave(0)
<br/>promoted_last_5years: A dummy variable, promoted(1), not promoted(0)
<br/>sales: A categorical variable assessing the department in which employee is working (sales,technical,support,IT, product,marketing, other)
<br/>salary: A 3-level categorical variable (low, medium, high)

###Data quality report
First of all we assess that there are no missing data with function is.na(myData) that we would not report in the code and and we perform basic summary statistic of the dataset.

```{r, echo=FALSE,eval=TRUE}

summary(myData)

```
<br/>
<hr/>
<br/>
<br/>From basic statistical analysis we can see that the overall satisfaction of the company is really low, at around 63% and that approximately 24% of the employees have left. 

<br/>This brings us to the following step: we would like to visualize better who are the employees that have decided to leave.

##Step 2: Data Visualization

<br/>

We will start our analysis looking more deeply at a subset composed of only the employees that have left. In particular we will analyse the distribution of employees across variables:

We cut database in the desired subset composed by 3.571 observations:

```{r, echo=FALSE,eval=TRUE}

employeesleft <- myData %>% filter(left==1)
nrow(employeesleft)

```
<br/>We plot first of all the most intuitive variables:
<br/>"Satisfaction Level/ Last Evaluation/ Average monthly hours""
```{r, echo=FALSE,eval=TRUE}

par(mfrow=c(1,3))

hist(employeesleft$satisfaction_level,col="#3090C7", main = "Satisfaction level") 
hist(employeesleft$last_evaluation,col="#3090C7", main = "Last evaluation")
hist(employeesleft$average_montly_hours,col="#3090C7", main = "Average monthly hours")

```

<br/>
<br/>
<br/>
<br/>

<br/>From the previous histograms we can make the following preliminar observations:

<br/>None of the distributions seems normal but we see peak at the ends of the histograms
<br> Regarding satisfaction level the distribution of employers that are leaving is quite polarized on employees that are medium on satisfaction.
<br/>Regarding employees evaluation those that leave seems either really good (>.9) or average.
<br/>Employees that leave seems to those who work a lot( >250 ) and  below average (<150)



<br/>We then look at the distribution in the categorical variables:
<br/Salary and Departments


```{r, echo=FALSE,eval=TRUE}

par(mfrow=c(1,3))
hist(employeesleft$Work_accident,col="#3090C7", main = "Work accident")
plot(employeesleft$salary,col="#3090C7", main = "Salary")

```
```{r, echo=FALSE,eval=TRUE}
par(oma=c(4,0,0,0))
plot(employeesleft$sales,col="#3090C7", main = "Departments", las=2)

```


<br/>From a first analysis of these last three histograms we can make the following observations:

<br/>  The frequency of work accident per se doesn't not mean a lot 
<br/>  Employees that left seems to have generally low salary.
<br/>  Employees that left comes mainly from sales, support and technical

From the preliminary analysis, we understand that we would like to focus our analys on employees that we consider most valuable but that are leaving.
We decide to set this criteria looking at the median value and choosing those that have worked for the company for more than 3 years, have good last evaluation results >0.72, and have performed more than 4 projects.
This group is composed by 3556 people (23,7% of employees)

```{r, echo=FALSE,eval=TRUE}
hr_valuable_people <- employeesleft %>% filter(last_evaluation >= 0.72 | time_spend_company >= 3 | number_project > 4)
  nrow(hr_valuable_people)
```
<br/>
<br/>
<br/>
<br/>
<br/>
<br/>As of part of the preliminary analysis we then perform an initial correlation analysis only with numerical variables the following group of employees:
1) all the employees
2) the valuable employees identified before
3) the employees that left 

```{r, echo=FALSE,eval=TRUE}
Allemployees <- myData %>% select(satisfaction_level:promotion_last_5years)
thecor <- cor(Allemployees)
cex.before <- par("cex")
corrplot(thecor, method = "circle" , type = "full", title = "Correlation Heat map of all data" , tl.cex = 1/par("cex"), cl.cex = 1/par("cex"), order = "original", number.font = 2 , tl.col = "red", tl.srt = 30, mar=c(0,0,2,0))
par(cex = cex.before)
```
<br/>
<br/>
<br/>

```{r, echo=FALSE,eval=TRUE}
hr_valuable_people <- myData %>% filter(last_evaluation >= 0.72 | time_spend_company >= 3  | number_project > 4)
hr_valuable_people2 <- hr_valuable_people %>% select(satisfaction_level, number_project: promotion_last_5years)
M <- cor(hr_valuable_people2)
cex.before <- par("cex")
corrplot(M, method = "circle" , type = "full", title = "Valuable people" , tl.cex = 1/par("cex"), cl.cex = 1/par("cex"), order = "original", number.font = 2 , tl.col = "red", tl.srt = 30, mar=c(0,0,2,0))
par(cex = cex.before)
```
<br/>
<br/>
<br/>
<br/>
<br/>

```{r, echo=FALSE,eval=TRUE}
thecor2 <- cor(employeesleft[c(1,2,3,4,5,6,8)])
corrplot(thecor2, method="circle" , title = "Correlation Heat map of employees who left" , tl.cex = 1/par("cex"), cl.cex = 1/par("cex"), order = "original", number.font = 2 , tl.col = "red", tl.srt = 30, mar=c(0,0,2,0))
```
<br/>
<br/>
<br/>
###OBSERVATIONS:
<br/> As a conclusion we can see how the only singnigicant correlation that we can find are across the subgroup of the employees that have left.



##Step 3: Principal Component Analysis

<br/>We then perform a more accurate correlation by first taking the entire database, then excluding what we can consider the dependent variable (left) and finally scaling the data.

```{r, echo=FALSE,eval=TRUE}
myData2 <- myData[,c(1,2,3,4,5,6,8,9,10)]
colnames(myData2)
myData2$sales <- as.factor(myData2$sales)
myData2$salary<-as.factor(myData2$salary)
myData2$salary<-ordered(myData2$salary,levels=c("low","medium","high"))
ProjectData <- data.matrix(myData2)
```


```{r, echo=FALSE,eval=TRUE}

MIN_VALUE=0.5
max_data_report = 10

colnames(ProjectData)

factor_attributes_used= c(1:9)

# Please ENTER the selection criterions for the factors to use. 
# Choices: "eigenvalue", "variance", "manual"
factor_selectionciterion = "eingenvalue"

# Please ENTER the desired minumum variance explained 
# (ONLY USED in case "variance" is the factor selection criterion used). 
minimum_variance_explained = 65  # between 1 and 100

# Please ENTER the number of factors to use 
# (ONLY USED in case "manual" is the factor selection criterion used).
manual_numb_factors_used = 4

# Please ENTER the rotation eventually used (e.g. "none", "varimax", "quatimax", "promax", "oblimin", "simplimax", and "cluster" - see help(principal)). Defauls is "varimax"
rotation_used="varimax"

factor_attributes_used = unique(sapply(factor_attributes_used,function(i) min(ncol(ProjectData), max(i,1))))
ProjectDataFactor=ProjectData[,factor_attributes_used]
ProjectDataFactor <- ProjectData <- data.matrix(ProjectDataFactor)
```


<br/>After having done the first  adjustments we can confirm that data are metric.
<br/>However we need to scale the data to have a more homogeneus dataset.

```{r echo=FALSE, tidy=TRUE}
knitr::kable(round(my_summary(ProjectDataFactor), 2))
```
```{r, echo=FALSE, tidy=TRUE}
ProjectDatafactor_scaled=apply(ProjectDataFactor,2, function(r) {if (sd(r)!=0) res=(r-mean(r))/sd(r) else res=0*r; res})
```
<br/>Here reported the summary statistics of the scaled dataset:

```{r echo=FALSE, tidy=TRUE}
knitr::kable(round(my_summary(ProjectDatafactor_scaled), 2))
```



<br> We can now once again perform correlation on those data and as we can see there are few variable showing strong pariwise corrrelation (+0.3)
```{r}
thecor = round(cor(ProjectDatafactor_scaled),2)
colnames(thecor)<-colnames(ProjectDatafactor_scaled)
rownames(thecor)<-colnames(ProjectDatafactor_scaled)

knitr::kable(round(thecor,2))

```

<br>Despite that, we would proceed with PCA ans after trying differerent combination of PCA ( manual, varimax , eigenvalue) we assess that best model is given by "eingenvalue".


```{r echo=FALSE, tidy=TRUE}
UnRotated_Results<-principal(ProjectDataFactor, nfactors=ncol(ProjectDataFactor), rotate="none",score=TRUE)
UnRotated_Factors<-round(UnRotated_Results$loadings,2)
UnRotated_Factors<-as.data.frame(unclass(UnRotated_Factors))
colnames(UnRotated_Factors)<-paste("Component",1:ncol(UnRotated_Factors),sep=" ")
```

```{r echo=FALSE, tidy=TRUE}
Variance_Explained_Table_results<-PCA(ProjectDataFactor, graph=FALSE)
Variance_Explained_Table<-Variance_Explained_Table_results$eig
Variance_Explained_Table_copy<-Variance_Explained_Table


rownames(Variance_Explained_Table) <- paste("Component", 1:nrow(Variance_Explained_Table))
colnames(Variance_Explained_Table) <- c("Eigenvalue", "Pct of explained variance", "Cumulative pct of explained variance")

knitr::kable(round(Variance_Explained_Table, 2))
```
```{r figure01, echo=FALSE, tidy=TRUE}
eigenvalues  <- Variance_Explained_Table[, "Eigenvalue"]
df           <- cbind(as.data.frame(eigenvalues), c(1:length(eigenvalues)), rep(1, length(eigenvalues)))
colnames(df) <- c("eigenvalues", "components", "abline")
ggplot(melt(df, id="components"), aes(x=components, y=value, colour=variable)) + geom_line()

```

```{r ,  echo=FALSE, tidy=TRUE}

Rotated_Results<-principal(ProjectDatafactor_scaled, nfactors=max(4), rotate=rotation_used,score=TRUE)
Rotated_Factors<-round(Rotated_Results$loadings,2)
Rotated_Factors<-as.data.frame(unclass(Rotated_Factors))
colnames(Rotated_Factors)<-paste("Component",1:ncol(Rotated_Factors),sep=" ")

sorted_rows <- sort(Rotated_Factors[,1], decreasing = TRUE, index.return = TRUE)$ix
Rotated_Factors <- Rotated_Factors[sorted_rows,]

knitr::kable(Rotated_Factors)
```

```{r echo=FALSE, tidy=TRUE}

Rotated_Factors_thres <- Rotated_Factors
Rotated_Factors_thres[abs(Rotated_Factors_thres) < MIN_VALUE]<-NA
colnames(Rotated_Factors_thres)<- colnames(Rotated_Factors)
rownames(Rotated_Factors_thres)<- rownames(Rotated_Factors)

knitr::kable(Rotated_Factors_thres)
```

<br>Based on eingvalue we can see that database can be reduced to three principal components that we try to interpret:

<br>Component 1 (number of project, monthly hours, last evaluation):  we will call this component "commitment"
<br>Component 2 : (salary, promotion) we can call this component "compensation"
<br>Component 3: (time spent at company, satisfaction) we can call this component "work-life balance"
<br>Component 4:  (department, work accident) we can call this component "role"

###OBSERVATIONS:
<br>PCA seems to be logically consistent in lowering the number of component to 4. We can infer that number of project, monthly hours, last evaluation ( commitment) is group most interrelated variables.
However given the limited number independent variables that we have in the dataset and the limited correlation across those variables we can conclude that PCA is not particularly useful in our case.

##Step 4) Futher comparative analysis on employees that left

<br>We than look more closely to employees that left but in this case we directly compare them to those who haven't left

```{r ,echo=FALSE, tidy=TRUE}
myData3 <- read.csv("DATA/HR_comma_sep.csv")
colnames(myData3) <- c("satisfaction","evaluation","project","hours","years","accident",
                  "left","promotion","sales","salary")

myData3$sales <- as.factor(myData3$sales)
myData3$salary <- factor(myData3$salary,levels=c("low","medium","high"),ordered=T)
des <- describeBy(myData3,list(left=myData3$left))
for(i in names(des)){
  des[[i]] <- as.data.frame(sapply(des[[i]],round,1))
}
kable(des[[1]])
kable(des[[2]])
```

Here we can have a overview of the data; for example, there are `r table(myData3$left)[2]` observations in `1` group, values of mean and median of `satisfaction` in `1` group is smaller than `0` group's. The correlation plot show some relationships of first 8 variables. Obviously, with lower satisfaction level, employees will have stronger desire to leave. If someone can deal with more projects and spend more time on work, he or she will obtain higher evaluation.

###Plot the two subset one against the other

we may find the evidence who will leave next.

```{r tableplot , echo=FALSE, tidy=TRUE}
require(tabplot)
myData3$left <- as.factor(myData3$left)
myData3$accident <- as.factor(myData3$accident)
myData3$promotion <- as.factor(myData3$promotion)
tableplot(myData3,sortCol=left)
```

Wile the table plot can show us enough features, We should do some more plots to analyse further

```{r echo=FALSE, tidy=TRUE}
g1 <- ggplot(myData3,aes(x=hours,group=left))+
  geom_histogram(aes(fill=left),alpha=.4)
g2 <- ggplot(myData3,aes(x=evaluation,group=left))+
  geom_histogram(aes(fill=left),alpha=.4)
g3 <- ggplot(myData3,aes(x=satisfaction,group=left))+
  geom_histogram(aes(fill=left),alpha=.4)
grid.arrange(g1,g2,g3,nrow=3)
```

```{r sat,echo=FALSE}
sat <- myData3 %>% select(satisfaction,left) %>% 
  filter(left=="1",satisfaction<.5) %>% 
  summarise(n()/nrow(myData3[myData3$left=="1",]))
sat <- paste(round(sat,4)*100,"%")
```


People with too few or too many hours per month on work tend to quit. The former may have little challenge in work and just get bored. On the contrary, the latter dont't want to spend much time on work. Employees who left get lower or higher last evaluation. Sense of satisfaction seems to be another factor; Satisfation level of `r sat` employees who left is less than 0.5.



In the former correlation plot, there is a kind of positive correlativity between `project` and `hours`, so the two variables maybe share the similar impact. People with few projets( less than 3) and too many projects( more than 5) tend to leave. In the left group, number of employees with three-year experience is 1586, almost half of the group. But, more than 50% employees with five-year experience quited, which is big loss for a company.

```{r ,echo=FALSE, tidy=TRUE}
mosaic(~accident+left+salary,data=myData3,
       highlighting = "left",highlighting_fill=c("lightblue","pink"),
       direction=c("h","v","h"))
```

Interestingly, employees with stronger desire to leave tend to have no accident in their work. Low salary also contributes to  resigning. Given variable `promotion`, the value's distribution is so uneven; we can see, only 319 out of 14999 employees have gotten promotion in the last five years. I will just eliminate the variable `promotion` in prediction process.

### Reasons of Employees Leaving?

Here are some reasons I've gotten from above analysis:

- Lower satisfication level is significant;

- Few or too many projects, little or too much time on work;

- Low salary;

- Three-to-six year for company is the risk time;

- Few chances to get promotion( I just guess).

### Who Are Valuable Employees And Why Do They Leave?

In my view, the employees with evaluation `>=0.72`, equal or more than 3-year-experience in the company and equal or more than  4 projects in hand can be considered to be experienced and valuable ones for a company. More than half of the experienced ones left. It seems that they suffer from:

- low level of satisfaction

- too much work

- no promotion

- low salary


```{r ,echo=FALSE, tidy=TRUE}
valuable <- myData3[myData3$evaluation>=.72 & myData3$years>=3 & myData3$project>=4,]
prop.table(table(valuable$left))
tableplot(valuable,sortCol=left,nBins=60)
ggplot(valuable,aes(x=left,y=satisfaction,fill=left))+
  geom_boxplot()+
  geom_point(position=position_jitter(width=.2),shape=1,alpha=.4)
```

We can find some employees with high sense of satisfaction also left in the box plot. The table plot shows they sustain the same problems with other left ones. Maybe there are still some reasons out of the dataset.


##Step 5: Prediction Model

It's time for us to model. We may get a good one to predict who will leave next.We will split the dataset into two parts, `train` and `test`. A classification tree will be built firstly, then forest.

```{r ,echo=FALSE, tidy=TRUE}
myData3 <- select(myData3,-promotion)
obs <- sample(1:nrow(myData3),10000)
train <- myData3[obs,]
test <- myData3[-obs,]

trControl <- trainControl(method="repeatedcv",
                          number=5,
                          repeats=3)
set.seed(112)
rpart_mdl <- train(left~.,data=train,
                   method="rpart",
                   trControl=trControl,
                   tuneGrid=data.frame(cp=c(.01,.02,.03)))

rpart_mdl
plot(as.party(rpart_mdl$finalModel))
```

A single tree can get a score like `r rpart_mdl$results[1,2]`, which is so unbelievable. The original dataset is so friendly. Some variables are more important in prediction as we can see from tree plot, such as, `satisfication`, `years`, `evaluation` and `hours`. Now we model a random forest.

```{r , echo=FALSE, tidy=TRUE}
set.seed(112)
rf_mdl <- randomForest(left~.,data=train,mtry=7,importance=T)

rf_pred <- predict(rf_mdl,test,type="response")
confusionMatrix(rf_pred,test$left)
```
##Step 7: Conclusion

So, `rf_mdl` works better, and which variable is the most important? From importance of variables, we can infer reasons that employees left, especially experienced and valuable ones.


```{r imp, echo=FALSE, tidy=TRUE}
importance <- data.frame(var=row.names(importance(rf_mdl)),importance=importance(rf_mdl)[,4])
ggplot(importance,aes(x=reorder(var,importance),y=importance))+
  geom_bar(stat="identity",aes(fill=importance))+
  coord_flip()+
  labs(title="Importance of Variables")
```



In our opinion Improving level of satisfaction is the key to keep employees remain. While, reasonable workload and working time, promotion and appropriate salary increasing can be useful measures to achieve the key. There is no doubt a valuable employee is important to a company, whether he or she is an experienced or potential one.


Our conclusion is 

